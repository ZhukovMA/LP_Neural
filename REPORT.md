# Отчет по лабораторной работе 
## по курсу "Искусственый интеллект"

## Нейросетям для распознавания изображений


### Студенты: 

| ФИО       | Роль в проекте                     | Оценка       |
|-----------|:-----------------------------------|--------------|
| Жуков М. А. | Подготовил датасет, обучил нейросети, написал отчет |          |

## Результат проверки

| Преподаватель     | Дата         |  Оценка       |
|-------------------|--------------|---------------|
| Сошников Д.В. |              |               |

> *Комментарии проверяющих (обратите внимание, что более подробные комментарии возможны непосредственно в репозитории по тексту программы)*

## Тема работы - Нейронные сети

Необходимо подготовить набор данных и построить несколько нейросетевых классификаторов: однослойная (многослойная) сеть и сверточная сеть для распознавания рукописных символов. В нашем 2-ом варианте - 5 первых символов греческого алфавита.

## Распределение работы в команде

Обязанности разбились так:

1. Жуков Максим - подготовка датасета, на основе подготовленных ранее рукописных символов. На выходе 2 датасета с тренировочными и тестовыми данными, разбитые на единичные картинки, обучение однослойно и многослойной сети, обучение сверточной сети.


## Подготовка данных

Пример фотографии исходных листков с рукописными символами:
![Концептуализация](https://github.com/ZhukovMA/LP_Neural/blob/main/zhukov.jpg)

## Загрузка данных

Для загрузки данны применялся класс ImageDataGenerator, который по указанному пути считывал изображения и разбевал их на batch по тензорам для всех классов.

## Обучение нейросети

### Полносвязная однослойная сеть

![](https://github.com/ZhukovMA/LP_Neural/blob/main/OL_model.png)
![](https://github.com/ZhukovMA/LP_Neural/blob/main/OL_accuracy.png)
![](https://github.com/ZhukovMA/LP_Neural/blob/main/OL_loss.png)

**Результаты**

Здесь помимо полносвязного слоя (Dense) были применен выравнивающий слой (Flatten). Как ни странно, но в этой модели результат тестовой выборки - 0,35, хоть и низкий, но максимальный на всех 3 моделях, что еще раз подтверждает несовершенность dataset, который описан в конечном выводе. Остальные параметры модели как batch_size и epochs брались со сверточной сети (так как она уже была закончена и подтвердила результатом на обучающей выборке).

### Полносвязная многослойная сеть

![](https://github.com/ZhukovMA/LP_Neural/blob/main/ML_model.png)
![](https://github.com/ZhukovMA/LP_Neural/blob/main/ML_accuracy.png)
![](https://github.com/ZhukovMA/LP_Neural/blob/main/ML_loss.png)

**Результаты**

Обучение многослойной модели также дало плохие результаты (но все равно это лучше чем сверточная модель) - 0,32. При создании модели были взяты почти те же слои (Faltten, Dropout, Batch_Normalization), а на место сверточных слоев пришли 3 полносвязных скрытых моделей. Стоит отметить, что эта модель стала раньше выдавать более высокие значения на тренировочной выборке, чем любая другая.

### Свёрточная сеть

![](https://github.com/ZhukovMA/LP_Neural/blob/main/C_model.png)
![](https://github.com/ZhukovMA/LP_Neural/blob/main/C_accuracy.png)
![](https://github.com/ZhukovMA/LP_Neural/blob/main/C_loss.png)

**Результаты**
Схема модели строилась с опорой на VGG19 (хотя число слоев и параметр depth уменьшенны). По общему шаблону использовалась схема активациия 'relu'. Слои: 18 сверточных (Convolution2D), 3 полносвязных (Dense), 1 выравнивающий (Flatten) и 1 Dropout (для исключений). Также стоит отметить использование Batch_normalization: без использования batch точность не превосходила 0,25 даже на тестовой выборке, когда с использованием показатели обучающей выборки координально изменились.

Однако и при примении batch не удалось получить хорошее значение для тестовой выборки (среднее значение - 0,29 , а максимальное ~ 0,5). При поиске возможностей улучшения моделей были попытки увеличения число сверточных слоев (до текущего), уменьшение batch_size с 64 до 32, и взято число epochs. Скорее всего проблема кроется в датасетах (описание этого будет в итоговом вывводе для всех моделей), ведь на тренировочной выборке были достигнуты очень хорошие результаты, а остальные изменения вели бы к еще большему переобучению модели.

## Выводы

В ходе данной лабораторной работы оказалось, что не так сложно написать модель, как подобрать наилучшие параметры, чтобы сеть правильно обучилась, и при этом не было переобучения (что у нас и получилось), и время обучения было приемлимым. Также я убедился в важности подготовки датасета, скорее всего из-за него и получились такие низкие резултаты. Уже после более тщательного просмотра все изображений, стало понятно, что использование одного цвета ручки, более эталонная запись символов (особенно это пригодилось на греческом алфавите, в котором много кривых).

Возвращаясь ко времени обучения, были попытки взять обучение модели (пока не было Batch_normalization) количеством, что привело к огромным временным затратам, но не дало никакого результата, что еще раз подтверждает, что правильно подобранная по слоям модель будет эффективнее громоздкой неоптимизированной модели.
